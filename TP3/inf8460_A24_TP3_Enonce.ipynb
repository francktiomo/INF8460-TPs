{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G9ZPAA_02tRR"
   },
   "source": [
    "##### INF8460 ‚Äì Traitement automatique de la langue naturelle - Automne 2024\n",
    "\n",
    "## TP3: G√©n√©ration automatique de mots-cl√©s (concepts) avec une architecture Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NWO5vIjG2tRS"
   },
   "source": [
    "## Identification de l'√©quipe:\n",
    "\n",
    "### Groupe de laboratoire:\n",
    "\n",
    "### Equipe num√©ro :\n",
    "\n",
    "### Membres:\n",
    "\n",
    "- membre 1 (% de contribution, nature de la contribution)\n",
    "- membre 2 (% de contribution, nature de la contribution)\n",
    "- membre 3 (% de contribution, nature de la contribution)\n",
    "\n",
    "* nature de la contribution: D√©crivez bri√®vement ce qui a √©t√© fait par chaque membre de l‚Äô√©quipe. Tous les membres sont cens√©s contribuer au d√©veloppement. Bien que chaque membre puisse effectuer diff√©rentes t√¢ches, vous devez vous efforcer d‚Äôobtenir une r√©partition √©gale du travail. Soyez pr√©cis ! N'indiquez pas seulement : travail r√©parti √©quitablement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FayMA0Z9Fxp"
   },
   "source": [
    "## 1. Objectif du TP\n",
    "\n",
    "Dans ce TP, vous allez devoir impl√©menter en `PyTorch` un mod√®le suivant l'architecture Transformer cr√©√©e initialement dans l'article \"[Attention Is All You Need](https://arxiv.org/pdf/1706.03762)\".\n",
    "<br>\n",
    "Votre t√¢che sera d'impl√©menter l'architecture du Transformer et d'entra√Æner un mod√®le de type Transformer √† g√©n√©rer les concepts cl√©s d'une phrase. Votre mod√®le prendra donc en entr√©e une phrase quelconque en anglais et produira en sortie une s√©quence de mots correspondant aux concepts les plus importants de la phrase. Par exemple :\n",
    "<br>\n",
    "\n",
    "**Entr√©e** : \"The dog jumped over the fence.\" <br>\n",
    "**Sortie attendue** : ['dog' 'jump' 'fence']\n",
    "\n",
    "Cette t√¢che a d'ailleurs plusieurs applications dans le monde r√©el allant de la g√©n√©ration de r√©sum√©s jusqu'√† la cr√©ation de bases de connaissances √† partir d'un texte.\n",
    "\n",
    "\n",
    "#### Requis et ressources utiles\n",
    "\n",
    "Il est fortement conseill√© de s'√™tre familiaris√© avec la librairie `PyTorch` avant d'entamer le TP. Par exemple, vous devriez savoir comment fonctionne un objet de type `nn.Module` et ce que la fonction `forward` de cet objet fait. Plusieurs tutoriels sont disponibles sur internet expliquant clairement le fonctionnement de la librairie. Voici certaines ressources qui peuvent √™tre utiles :\n",
    "- [Build The Neural Network](https://pytorch.org/tutorials/beginner/basics/buildmodel_tutorial.html)\n",
    "- [PyTorch Tutorial](https://github.com/yunjey/pytorch-tutorial?tab=readme-ov-file)\n",
    "\n",
    "\n",
    "## 2. Jeu de donn√©es\n",
    "\n",
    "Le jeu de donn√©es est compos√© de paires (phrase, concepts) o√π une phrase est mentionn√©e et la liste des concepts les plus importants de la phrase est indiqu√©e. Le jeu de donn√©es est divis√© en ensembles d'entra√Ænement, de validation et de test :\n",
    "- Entra√Ænement : 4500 examples (train.csv)\n",
    "- Validation : 500 examples (val.csv)\n",
    "- Test : 500 examples (test.csv)\n",
    "\n",
    "Pour une m√™me s√©quence de concepts, il y a 3 phrases pouvant √™tre associ√©es √† ces concepts. Par exemple, pour les concepts : ['cat' 'laptop' 'lie'], les phrases suivantes y sont associ√©es :\n",
    "- A cat lying on a laptop\n",
    "- A large cat lies down next to a laptop.\n",
    "- There is a cat lying on top of the laptop.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "26Qe_-7z25EE"
   },
   "source": [
    "## 3. LIBRAIRIES PERMISES\n",
    "- Jupyter notebook\n",
    "- PyTorch\n",
    "- nltk\n",
    "- transformers\n",
    "- pandas\n",
    "- matplotlib\n",
    "- numpy\n",
    "- Huggingface\n",
    "\n",
    "\n",
    "Pour toute autre librairie, demandez √† votre charg√© de laboratoire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ssmtsfa12_CQ"
   },
   "source": [
    "## 4. INFRASTRUCTURE\n",
    "\n",
    "- Vous avez acc√®s aux GPU du local L-4818. Dans ce cas, vous devez utiliser le dossier temp (voir le tutoriel VirtualEnv.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hn5uuxc59Fxp"
   },
   "source": [
    "## 5. √âTAPES DU TP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eNlUvpo32tRS"
   },
   "outputs": [],
   "source": [
    "# Pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as O\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# ML\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# System\n",
    "from dataclasses import dataclass\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Huggingface\n",
    "from tokenizers import CharBPETokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHahteTL9Fxq"
   },
   "source": [
    "Il vous faudra peut-√™tre t√©l√©charger ce module de `nltk`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xAjX6Fzz9MjK",
    "outputId": "e46e2d3c-76d6-44e6-873b-c9ceca5c2927"
   },
   "outputs": [],
   "source": [
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIwDxV0G2tRS"
   },
   "source": [
    "### 1. Charger les donn√©es (2 points)\n",
    "\n",
    "La premi√®re √©tape va √™tre de charger les donn√©es des fichiers d'entra√Ænement et d'√©valuation. Pour cela, nous allons utiliser la classe `Dataset` de `PyTorch`. Compl√©tez les fonctions de cette classe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Ppkz_YFb2tRS"
   },
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, file_path: str = None, data: pd.DataFrame = None):\n",
    "\n",
    "        if file_path is None and data is None:\n",
    "            raise Exception('A file path or a dataframe must be passed to create a CustomDataset')\n",
    "\n",
    "        if file_path != None:\n",
    "            self.data = pd.read_csv(file_path)\n",
    "        else:\n",
    "            self.data = data.copy()\n",
    "\n",
    "        \n",
    "        # TODO : Enlever les donn√©es qui sont nulles et\n",
    "        # assigner le r√©sultat dans la variable data\n",
    "\n",
    "        # END TODO\n",
    "\n",
    "    def __len__(self):\n",
    "        # TODO : Retourne le nombre de donn√©es\n",
    "        pass\n",
    "        # END TODO\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        # TODO : Retourne l'√©l√©ment √† l'index \"idx\"\n",
    "        pass\n",
    "        # END TODO\n",
    "\n",
    "    def get_column(self, column: str):\n",
    "        # TODO : Retourne toutes les rang√©es de la colonne \"column\"\n",
    "        pass\n",
    "        # END TODO\n",
    "\n",
    "    def get_batch(self, idx: int, size):\n",
    "        # TODO : Retourne toutes les rang√©es de l'index \"idx\" √† \"idx\" + \"size\"\n",
    "        pass\n",
    "        # END TODO\n",
    "\n",
    "    def transform(self, function):\n",
    "        self.data = self.data.apply(function)\n",
    "\n",
    "    def sample(self, nb_samples, random_state=42):\n",
    "        return self.data.sample(n=nb_samples, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2CWT0dXF2tRT"
   },
   "outputs": [],
   "source": [
    "root = 'data/'\n",
    "\n",
    "train_dataset = CustomDataset(root + 'train.csv')\n",
    "test_dataset = CustomDataset(root + 'test.csv')\n",
    "val_dataset = CustomDataset(root + 'val.csv')\n",
    "\n",
    "INPUT_COLUMN = 'sentence'\n",
    "OUTPUT_COLUMN = 'concepts'\n",
    "MAX_LENGTH = 16\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu' # Vous pouvez changer sur quelle machine les calculs seront effectu√©s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rv_Bjpg92tRT",
    "outputId": "32dc143c-1187-4474-f617-927c003956c9"
   },
   "outputs": [],
   "source": [
    "print('Training set size : ', len(train_dataset))\n",
    "print('Validation set size : ', len(val_dataset))\n",
    "print('Testing set size : ', len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-HmDFdv2tRT"
   },
   "source": [
    "### 2. Statistiques (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p92b0tB79Fxr"
   },
   "source": [
    "Avant d'entamer la cr√©ation du mod√®le, il est important de se familiariser avec les donn√©es et d'√©valuer la taille des donn√©es avec lesquelles nous travaillons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5oOmXpLE2tRT"
   },
   "source": [
    "#### 2.1 Histogramme du nombre de caract√®res (2 points)\n",
    "\n",
    "Compl√©tez la m√©thode `show_histogram_nb_characters` qui affiche un histogramme de la distribution du nombre de caract√®res des exemples de la colonne pass√©e en param√®tre. L'axe des abscisses devra repr√©senter le nombre de caract√®res et l'axe des ordonn√©es le nombre de documents. Utilisez des bacs (bins) de 20 pour l'histogramme. Affichez ensuite la distribution du nombre de caract√®res sur l'ensemble d'entra√Ænement pour la colonne 'sentence' et la colonne 'concepts'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "id": "38GHBsHZ2tRT",
    "outputId": "f2fb7bde-a7b1-4c3c-e4dc-042d16788123"
   },
   "outputs": [],
   "source": [
    "def show_histogram_nb_chararacters(dataset: CustomDataset, column: str):\n",
    "\n",
    "    \"\"\"\n",
    "    Affiche la distribution de la colonne pass√© en param√®tre. L'histogramme doit contenir un titre et des titres sur les axes\n",
    "\n",
    "    dataset: Dataset contenant plusieurs colonne dont la colonne dont les statistiques doivent √™tre affich√©es\n",
    "    column: Colonne √† afficher\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    pass\n",
    "    # END TODO\n",
    "\n",
    "\n",
    "show_histogram_nb_chararacters(train_dataset, INPUT_COLUMN)\n",
    "show_histogram_nb_chararacters(train_dataset, OUTPUT_COLUMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zOOUTrZU2tRT"
   },
   "source": [
    "#### 2.2 Histogramme du nombre de mots par document (2 points)\n",
    "\n",
    "De la m√™me mani√®re, compl√©tez la m√©thode `show_histogram_nb_words` qui affiche un histogramme de la distribution du nombre de mots des exemples de la colonne pass√©e en param√®tre. Utilisez des bacs (bins) de 20 pour l'histogramme. Affichez ensuite la distribution du nombre de mots sur l'ensemble d'entra√Ænement pour les colonnes \"sentence\" et \"concepts\". Dans le cas de la colonne 'concepts', assurez-vous que les '[', ']' et les apostrophes ne soient pas consid√©r√©es comme des mots.\n",
    "\n",
    "Indice : Utilisez la m√©thode word_tokenize() de nltk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 927
    },
    "id": "wq97r1Yc2tRT",
    "outputId": "ae506763-efeb-4385-eb43-5e3b04be15c2"
   },
   "outputs": [],
   "source": [
    "def show_histogram_nb_words(dataset: CustomDataset, column: str):\n",
    "\n",
    "    \"\"\"\n",
    "    Affiche la distribution de la colonne pass√© en param√®tre du dataset. L'histogramme doit contenir un titre et des titres sur les axes\n",
    "\n",
    "    dataset: Dataset contenant plusieurs colonne dont la colonne dont les statistiques doivent √™tre affich√©es\n",
    "    column: Colonne √† afficher\n",
    "\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    pass\n",
    "    # END TODO\n",
    "\n",
    "show_histogram_nb_words(train_dataset, INPUT_COLUMN)\n",
    "show_histogram_nb_words(train_dataset, OUTPUT_COLUMN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fKAGNVSe2tRT"
   },
   "source": [
    "#### 2.3 Commentez les graphiques (1 points)\n",
    "1. Est-ce que les distributions suivent des distributions normales ?\n",
    "2. Qu'observez-vous de sp√©cial sur la distribution du nombre de mots de la colonne \"concepts\" ? Pourquoi est-elle ainsi ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EDnurIIU2tRT"
   },
   "source": [
    "### 3. Segmentation (Tokenization) (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BU1nJ3xT2tRT"
   },
   "source": [
    "#### 3.1 Entra√Ænement du segmenteur (tokenizer) (3 points)\n",
    "La fonction `word_tokenize()` de `nltk` est utile pour rapidement ressortir des statistiques, mais elle reste tr√®s g√©n√©rale et inefficace. Afin de r√©duire le plus possible la taille du vocabulaire, nous allons entra√Æner notre propre Tokenizer en nous basant sur l'algorithme BPE (Byte Pair Encoding). L'algorithme BPE est une m√©thode de compression de texte qui permet de cr√©er des tokenizers efficaces en regroupant les caract√®res les plus fr√©quents. Il commence par diviser le texte en caract√®res uniques, puis it√©rativement fusionne les paires de caract√®res les plus fr√©quentes en nouveaux symboles. Ce processus continue jusqu'√† atteindre un nombre pr√©d√©fini de jetons (tokens), permettant ainsi de g√©rer des vocabulaires de diff√©rentes tailles de mani√®re flexible et efficace. √Ä l'aide de BPE, nous allons transformer les mots en nombres pour pouvoir les passer au Transformer par la suite. Chaque jeton (token) sera associ√© √† un nombre correspondant √† l'indice du jeton dans le vocabulaire. Une s√©quence en entr√©e sera ainsi repr√©sent√©e comme une s√©quence de nombres.\n",
    "\n",
    "De plus, des jetons sp√©ciaux seront ajout√©s au tokenizer pour sp√©cifier le d√©but d'une phrase (begin-of-sequence : `[BOS]`) et la fin d'une phrase (end-of-sequence : `[EOS]`). Un jeton de padding `[PAD]` sera utilis√© afin de s'assurer que toutes les phrases ont la m√™me taille. Finalement, un jeton `[UNK]` sera utilis√© pour les jetons inconnus.\n",
    "\n",
    "Pour cr√©er ce vocabulaire √† l'aide de l'algorithme BPE, il faudra entra√Æner le tokenizer sur notre ensemble d'entra√Ænement pour qu'il puisse encoder efficacement les mots qui y sont pr√©sents. L'entra√Ænement est d√©j√† impl√©ment√© par la classe `ByteLevelBPYTokenizer` de la librairie `tokenizers`.\n",
    "\n",
    "Vous n'avez qu'√† compl√©ter la fonction :\n",
    "- `data_generator` qui retourne un g√©n√©rateur d'un lot (batch) de texte provenant de l'ensemble d'entra√Ænement. Pour chaque lot, les colonnes \"sentence\" et \"concepts\" sont concat√©n√©es pour former la cha√Æne finale qui sera envoy√©e au tokenizer. Les lots sont utiles lors de l'entra√Ænement pour acc√©l√©rer l'entra√Ænement en parall√©lisant le tout et en √©vitant de charger tout notre ensemble d'entra√Ænement en m√©moire directement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "id": "T7ouSYOn9Fxr"
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 8000\n",
    "special_tokens = ['[PAD]', '[BOS]', '[EOS]', '[UNK]']\n",
    "base_tokenizer = CharBPETokenizer(unk_token=special_tokens[-1])\n",
    "tokenizer_batch_size = 64\n",
    "\n",
    "def data_generator():\n",
    "    \"\"\"\n",
    "    Generateur qui retourne un lot (batch) de texte provenant de l'ensemble d'entra√Ænement.\n",
    "    Pour chaque √©l√©ment d'un lot, les colonnes \"target\" et \"concepts\" sont concat√©n√©e.\n",
    "    \"\"\"\n",
    "    for i in range(0, len(train_dataset), tokenizer_batch_size):\n",
    "\n",
    "        # TODO\n",
    "        text = None # Mettez le lot dans la variable text\n",
    "        # END TODO\n",
    "\n",
    "        yield text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JS-nxpFN9Fxr"
   },
   "source": [
    "Vous devez maintenant compl√©ter la fonction :\n",
    "- `train_tokenizer` qui prend en param√®tre un tokenizer pour entra√Æner ce dernier √† l'aide de la fonction [train_from_iterator](https://github.com/huggingface/tokenizers/blob/main/bindings/python/py_src/tokenizers/implementations/byte_level_bpe.py). Le g√©n√©rateur de donn√©es (data_generator) est envoy√© √† la fonction train_from_iterator tout comme la taille du vocabulaire, les jetons sp√©ciaux et la fr√©quence minimum d'une s√©quence pour la consid√©rer comme un jeton. Indiquez explicitement une fr√©quence minimum de 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "id": "Xhy4Axwq9Fxr"
   },
   "outputs": [],
   "source": [
    "\n",
    "from tokenizers import AddedToken, Tokenizer, decoders, pre_tokenizers, processors, trainers\n",
    "\n",
    "def train_tokenizer(tokenizer: CharBPETokenizer):\n",
    "    \"\"\"\n",
    "    Entra√Æne le tokenizer pass√© en param√®tre en appelant la fonction train_from_iterator\n",
    "    et en sp√©cifiant le g√©n√©rateur de donn√©e (data_generator), la taille du vocabulaire,\n",
    "    les jetons sp√©ciaux et une fr√©quence minimum de 2 (indiquez le explicitement)\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    pass\n",
    "    # END TODO\n",
    "\n",
    "train_tokenizer(base_tokenizer)\n",
    "\n",
    "# On ajoute les jetons de d√©but, de fin de phrase et de jeton inconnu\n",
    "bos_token_id = base_tokenizer.token_to_id(\"[BOS]\")\n",
    "eos_token_id = base_tokenizer.token_to_id(\"[EOS]\")\n",
    "unk_token_id = base_tokenizer.token_to_id(\"[UNK]\")\n",
    "\n",
    "# On applique un template au tokenizer pour qu'il ajoute\n",
    "# les jetons au d√©but et √† la fin de chaque phrase\n",
    "base_tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=f\"[BOS]:0 $A:0 [EOS]:0\",\n",
    "    special_tokens=[\n",
    "        (\"[BOS]\", bos_token_id),\n",
    "        (\"[EOS]\", eos_token_id),\n",
    "    ],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "goTLCnvu9Fxr"
   },
   "source": [
    "Nous allons maintenant transformer notre tokenizer pour qu'il soit compatible avec l'API de la librairie Huggingface. Cela permettra entre autre de faciliter les appels de m√©thodes pour modifier facilement nos entr√©es. Par exemple, avec l'API de Huggingface, nous pouvons simplement appeler la m√©thode `tokenize` pour diviser une s√©quence en jetons (`Welcome` -> `[W, el, come]`). De la m√™me mani√®re, l'API nous permet d'appeler les m√©thodes `encode` et `decode` pour transformer une cha√Æne de caract√®res en s√©quences d'indices de jetons (`Welcome` -> `[36, 170, 664]`) et inversement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "id": "lFU2lsU52tRU"
   },
   "outputs": [],
   "source": [
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "tokenizer = PreTrainedTokenizerFast(tokenizer_object=base_tokenizer._tokenizer, truncation=True)\n",
    "tokenizer.add_special_tokens({\n",
    "    'pad_token': \"[PAD]\",\n",
    "    'bos_token': \"[BOS]\",\n",
    "    'eos_token': \"[EOS]\",\n",
    "    'unk_token': \"[UNK]\"\n",
    "});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I1HQ12tm9Fxr"
   },
   "source": [
    "Testons maintenant notre tokenizer sur une phrase de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RXwsa-s92tRU",
    "outputId": "f3aed644-46da-47f1-d5e7-22f0c0dc9488"
   },
   "outputs": [],
   "source": [
    "test_input = 'Welcome ! The boat arrived at the station ! ü§ó '\n",
    "print(test_input, '\\n')\n",
    "print(tokenizer.tokenize(test_input))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M9VPYkgF9Fxr"
   },
   "source": [
    "Observons maintenant la m√™me phrase, mais une fois encod√©e en nombre. D√©codons-la ensuite √† partir de sa version encod√©e pour voir si nous retrouvons la phrase initiale.\n",
    "\n",
    "- La m√©thode `encode` permet de transformer une s√©quence de mots en s√©quence de nombres correspondant aux indices des diff√©rents jetons de la phrase dans le vocabulaire\n",
    "- La m√©thode `decode` permet de transformer une s√©quence d'indices de jetons en phrase lisible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OGYIbuYw2tRU",
    "outputId": "415ff727-6e82-46c9-a969-e52de73618ad"
   },
   "outputs": [],
   "source": [
    "print('Texte initial : ', test_input)\n",
    "print('=' * 100)\n",
    "print('Texte encod√© : ', tokenizer.encode(test_input))\n",
    "print('Texte d√©cod√© : ', tokenizer.decode(tokenizer.encode(test_input)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bgAOmSe-9Fxr"
   },
   "source": [
    "#### 3.2 Que remarquez-vous dans la version segment√©e (tokenized) de la phrase de test ? Quelles sont les diff√©rences entre la phrase initiale et la phrase d√©cod√©e ? Pourquoi ? (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJt5coCQ9Fxs"
   },
   "source": [
    "### 4. Transformer (28 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P2qoKbLl9Fxs"
   },
   "source": [
    "Il est maintenant le temps de construire les couches du Transformer. Son architecture globale est pr√©sent√©e dans la figure suivante. :\n",
    "\n",
    "![Transformer](images/transformer.png)\n",
    "\n",
    "Les couches sont d√©finies pour vous et vous n'avez qu'√† compl√©ter, √† moins d'indication contraire, la fonction `forward` de chacune des classes qui prend un tenseur en entr√©e et effectue une transformation sur celui-ci pour g√©n√©rer une sortie. Cette transformation varie en fonction de chaque classe. Une description de ce que doit faire la classe est indiqu√©e √† chaque √©tape. Lorsque la fonction `forward` est complexe, une figure est fournie pour vous guider. Attention, ne changez pas le constructeur ou le nom de la classe !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hxIkdQnH9Fxs"
   },
   "source": [
    "#### 4.1 Positional Embedding\n",
    "Le Transformer encode l'information de l'ordre des mots dans les plongements des mots. Des plongements de position sont calcul√©s et ils sont ajout√©s aux plongements de contexte. Dans ce cas-ci la classe `PositionalEmbedding` vous est donn√©e."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {
    "id": "jWGyCzdH2tRU"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    Cette classe a √©t√© prise de l'impl√©mentation originale du papier 'Attention Is All You Need'\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, max_seq_length, embedding_dim):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.embedding_dim = embedding_dim\n",
    "\n",
    "        pe = torch.zeros(max_seq_length, self.embedding_dim)\n",
    "        position = torch.arange(0, max_seq_length, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.embedding_dim, 2).float() * -(math.log(10000.0) / self.embedding_dim))\n",
    "\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x + self.pe[:, :x.size(1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qGaeIr1r9Fxs"
   },
   "source": [
    "#### 4.2 Attention (6 points)\n",
    "Le m√©canisme d'attention est le coeur de l'architecture du Transformer. Il permet notamment la parall√©lisation de l'entra√Ænement tout en garantissant un lien direct entre tous les jetons. Vous devrez impl√©menter la fonction `scaled_dot_product_attention` qui effectue le calcul principal derri√®re le m√©canisme d'attention. Cette fonction prend en entr√©e les tenseurs `Q`, `K`, `V` et effectue le calcul suivant :\n",
    "$$A = \\texttt{softmax}\\Big( \\frac{Q K^T \\odot M}{\\texttt{head\\_dim}} \\Big) V$$\n",
    "o√π $M$ est le masque d'attention qui doit √™tre appliqu√©. Plus de d√©tails sont indiqu√©s dans la description de la fonction √† propos du masque. Par rapport au tenseur de cl√©s, vous remarquerez dans la formule qu'une transpos√©e est appliqu√©e sur ce tenseur. √âtant donn√© qu'un tenseur poss√®de plus que 2 dimensions, il est important de sp√©cifier quelles dimensions seront transpos√©es dans le tenseur. Dans notre cas, il s'agit des dimensions correspondants aux jetons de la s√©quence et aux plongements des jetons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "id": "lVVuGLFY2tRU"
   },
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, model_dim, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert model_dim % num_heads == 0, \"La dimension du mod√®le doit √™tre divisible par le nombre de t√™tes d'attention\"\n",
    "\n",
    "        self.model_dim = model_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = model_dim // num_heads\n",
    "\n",
    "        self.wq = nn.Linear(model_dim, model_dim) # Query\n",
    "        self.wk = nn.Linear(model_dim, model_dim) # Key\n",
    "        self.wv = nn.Linear(model_dim, model_dim) # Value\n",
    "        self.wo = nn.Linear(model_dim, model_dim) # Output\n",
    "\n",
    "        self.mask_value = -1e9\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
    "        \"\"\"\n",
    "        Calcule les plongements d'attention en utilisant la formule\n",
    "\n",
    "        attn = softmax(Q * K^T @ mask / head_dim) * V\n",
    "\n",
    "        Args :\n",
    "            Q : plongements des queries\n",
    "            Taille : [batch_size, num_heads, seq_length, head_dim]\n",
    "\n",
    "            K : plongements des keys\n",
    "            Taille : [batch_size, num_heads, seq_length, head_dim]\n",
    "\n",
    "            V : plongement des values\n",
    "            Taille : [batch_size, num_heads, seq_length, head_dim]\n",
    "\n",
    "            mask : Masque d'attention qui doit √™tre appliqu√© avant le softmax pour que\n",
    "            les jetons ne portent pas leur attention sur certains jetons. Le masque est\n",
    "            notamment utilis√© dans le d√©codeur pour s'assurer que le transformer n'ait\n",
    "            pas acc√®s aux futurs jetons lorsqu'il essaie de pr√©dire le prochain jeton.\n",
    "            Il contient des valeurs 0 ou 1. Une valeur de 0 √† la position i,j indique\n",
    "            que pour le jeton i, le jeton j doit √™tre masqu√©. Pour masquer la valeur,\n",
    "            il suffit de mettre une valeur tr√®s petite (self.mask_value) √† l'indice i,j.\n",
    "            La fonction masked_fill de PyTorch pourrait √™tre utile\n",
    "            Taille : [1, seq_length, seq_length]\n",
    "\n",
    "        Returns :\n",
    "        R√©sultat du calcul d'attention de taille [batch_size, num_heads, seq_length, head_dim]\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        pass\n",
    "        # END TODO\n",
    "\n",
    "    def split_heads(self, x):\n",
    "        \"\"\"\n",
    "        S√©pare une entr√©e sur plusieurs t√™tes d'attention\n",
    "\n",
    "        Args :\n",
    "            x : Tenseur d'entr√©e\n",
    "            Taille : [batch_size, seq_length, model_dim]\n",
    "\n",
    "        Returns :\n",
    "        Tenseur s√©par√© sur plusieurs t√™tes d'attention\n",
    "        Taille : [batch_size, num_heads, seq_length, head_dim]\n",
    "        \"\"\"\n",
    "        return x.view(x.shape[0], x.shape[1], self.num_heads, self.head_dim).transpose(1, 2)\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        \"\"\"\n",
    "        Combine une entr√©e √† travers les t√™tes d'attention\n",
    "\n",
    "        Args :\n",
    "            x : Tenseur d'entr√©e\n",
    "            Taille : [batch_size, num_heads, seq_length, head_dim]\n",
    "\n",
    "        Returns :\n",
    "        Tenseur s√©par√© sur plusieurs t√™tes d'attention\n",
    "        Taille : [batch_size, seq_length, model_dim]\n",
    "        \"\"\"\n",
    "        return x.transpose(1, 2).contiguous().view(x.shape[0], x.shape[2], self.model_dim)\n",
    "\n",
    "    def forward(self, queries, keys, values, mask=None):\n",
    "        \"\"\"\n",
    "        Ex√©cute le m√©canisme d'attention √† travers plusieurs t√™tes\n",
    "        d'attention\n",
    "\n",
    "        Args :\n",
    "            queries : plongements des queries\n",
    "            Taille : [batch_size, seq_length, model_dim]\n",
    "\n",
    "            keys : plongements des keys\n",
    "            Taille : [batch_size, seq_length, model_dim]\n",
    "\n",
    "            values : plongement des values\n",
    "            Taille : [batch_size, seq_length, model_dim]\n",
    "\n",
    "            mask : mask qui sera appliqu√©\n",
    "            Taille : [1, seq_length, seq_length]\n",
    "\n",
    "        Returns :\n",
    "        Tenseur contenant les plongements finaux de chaque indice de\n",
    "        la s√©quence\n",
    "        Taille : [batch_size, seq_length, model_dim]\n",
    "        \"\"\"\n",
    "        Q = self.split_heads(self.wq(queries))\n",
    "        K = self.split_heads(self.wk(keys))\n",
    "        V = self.split_heads(self.wv(values))\n",
    "\n",
    "        attn_output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
    "        return self.wo(self.combine_heads(attn_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4RmDSGKW9Fxs",
    "outputId": "9ea294a3-141f-4c45-cb7c-f8fe672dbd47"
   },
   "outputs": [],
   "source": [
    "def test_attention():\n",
    "    batch_size = 1\n",
    "    seq_length = 4\n",
    "    model_dim = 6\n",
    "    num_heads = 2\n",
    "\n",
    "    torch.random.manual_seed(42)\n",
    "    attention = MultiHeadAttention(model_dim=model_dim, num_heads=num_heads)\n",
    "    inputs = torch.randint(0, 10, (batch_size, seq_length, model_dim), dtype=torch.float32)\n",
    "    mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1))\n",
    "    print(attention.forward(inputs, inputs, inputs, mask=mask).detach())\n",
    "\n",
    "test_attention()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LYhfAMYn9Fxv"
   },
   "source": [
    "Sortie attendue :\n",
    "```\n",
    "tensor([[[-0.6789,  2.8487,  2.8666, -1.1983,  3.2915,  0.5359],\n",
    "         [-0.3437,  3.1045,  3.0715, -1.1405,  3.2748,  0.3843],\n",
    "         [-2.5825,  4.2728, -0.0114, -2.2663, -0.0922, -2.4731],\n",
    "         [-2.5749,  4.2706, -0.0056, -2.2613, -0.0874, -2.4694]]])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWlUCL9J9Fxv"
   },
   "source": [
    "#### 4.3 Encodeur (6 points)\n",
    "L'encodeur du Transformer encode la s√©quence d'entr√©e dans des vecteurs de contexte avant d'envoyer ces vecteurs de contexte au d√©codeur pour qu'ils puissent √™tre utilis√©s pour g√©n√©rer la s√©quence de sortie. Vous n'avez qu'√† compl√©ter les fonctions `forward` des classes `TransformerFeedForward`, `EncoderLayer` et `Encoder`. L'architecture d'une couche d'encodeur `EncoderLayer` est d√©crite dans la figure suivante :\n",
    "\n",
    "![EncoderLayer](images/encoder_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GXmZRBKw9Fxv"
   },
   "source": [
    "La classe `TransformerFeedForward` repr√©sente une couche simple de r√©seaux de neurones avec la fonction d'activation `ReLU` qui sera pr√©sente dans l'encodeur. Son architecture est d√©crite dans la figure suivante :\n",
    "\n",
    "![TransformerFeedForward](images/transformer_feed_forward.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "id": "NtiCsab89Fxv"
   },
   "outputs": [],
   "source": [
    "class TransformerFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dim: int, ff_dim: int) -> None:\n",
    "        super(TransformerFeedForward, self).__init__()\n",
    "        self.model_dim = model_dim\n",
    "        self.ff_dim = ff_dim\n",
    "\n",
    "        self.ff1 = nn.Linear(model_dim, ff_dim)\n",
    "        self.ff2 = nn.Linear(ff_dim, model_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Applique les deux couches lin√©aires (ff) cons√©cutivement avec la fonction\n",
    "        d'activation ReLU apr√®s chaque couche lin√©aire\n",
    "\n",
    "        Args :\n",
    "            x : Tenseur d'entr√©e de taille [batch_size, model_dim]\n",
    "\n",
    "        Returns :\n",
    "        Tenseur apr√®s √™tre pass√© √† travers les couches lin√©aires de taille\n",
    "        [batch_size, model_dim]\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        pass\n",
    "        # END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOdPH-wA9Fxv"
   },
   "source": [
    "La classe `EncoderLayer` repr√©sente une seule couche qui applique le m√©canisme d'attention sur la sortie de la couche pr√©c√©dente. Dans le cas de la premi√®re couche d'encodeur, il s'agit simplement de la couche de plongements des jetons. Par la suite, le r√©sultat du m√©canisme d'attention est normalis√© et envoy√© √† une couche de r√©seau de neurones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "id": "WM-OxKRy2tRU"
   },
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dim: int, ff_dim: int, dropout_rate: int = 0.3, num_heads=8) -> None:\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(model_dim=model_dim, num_heads=num_heads)\n",
    "        self.feed_forward = TransformerFeedForward(model_dim=model_dim, ff_dim=ff_dim)\n",
    "        self.attention_layer_norm = nn.LayerNorm(model_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(model_dim)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, encoder_mask: torch.Tensor = None):\n",
    "        \"\"\"\n",
    "        Applique la couche d'attention, de normalisation et de r√©seau de neurones\n",
    "        sur l'entr√©e\n",
    "\n",
    "        Args :\n",
    "            x : Tenseur d'entr√©e de l'encodeur correspondant √† la s√©quence d'entr√©e\n",
    "            Taille : [batch_size, seq_length, model_dim]\n",
    "\n",
    "        Returns :\n",
    "        Tenseur apr√®s avoir appliqu√©s les couches de taille [batch_size, seq_length,\n",
    "        model_dim]\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        pass\n",
    "        # END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktqwGE8r9Fxv"
   },
   "source": [
    "La classe `Encoder` correspond √† une suite de plusieurs couches d'encodeurs. La fonction `forward` de cette classe doit appeler chacune des couches (`EncoderLayer`) une √† la suite de l'autre en passant √† la couche courante la sortie de la couche pr√©c√©dente. Le masque est partag√© entre toutes les couches `EncoderLayer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "id": "75n3oqTP2tRV"
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers: int, model_dim: int, ff_dim: int, dropout_rate: int = 0.3, num_heads=8) -> None:\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [EncoderLayer(\n",
    "                model_dim=model_dim,\n",
    "                ff_dim=ff_dim,\n",
    "                dropout_rate=dropout_rate,\n",
    "                num_heads=num_heads,\n",
    "            ) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, encoder_mask: torch.LongTensor = None):\n",
    "        \"\"\"\n",
    "        Applique toutes les couches d'encodeur cons√©cutivement\n",
    "\n",
    "        Args :\n",
    "            x : Tenseur d'entr√©e de l'encodeur correspondant √† la s√©quence d'entr√©e\n",
    "            Taille : [batch_size, seq_length, model_dim]\n",
    "\n",
    "            encoder_mask : Tenseur contenant le masque qui sera utilis√© par l'encodeur\n",
    "            pour cacher certains jetons (notamment les jetons [PAD])\n",
    "        Returns :\n",
    "        Tenseur apr√®s avoir appliqu√©s les couches dans l'encodeur de taille\n",
    "        [batch_size, seq_length, model_dim]\n",
    "        \"\"\"\n",
    "        # TODO \n",
    "        pass\n",
    "        # END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBfB3AjY9Fxw"
   },
   "source": [
    "#### 4.4 D√©codeur (8 points)\n",
    "Le d√©codeur est la partie du Transformer qui g√©n√®re la s√©quence de sortie en prenant le contexte de la s√©quence d'entr√©e et les jetons qui ont √©t√© g√©n√©r√©s pr√©c√©demment. De la m√™me mani√®re que l'encodeur, la classe `DecoderLayer` repr√©sente une seule couche de d√©codeur. L'architecture du `DecoderLayer` est pr√©sent√©e dans la figure suivante :\n",
    "\n",
    "![DecoderLayer](images/decoder_layer.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "id": "Fj8tGjvg2tRV"
   },
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, model_dim: int, ff_dim: int, dropout_rate: int = 0.3, num_heads=8) -> None:\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(model_dim=model_dim, num_heads=num_heads)\n",
    "        self.cross_attention = MultiHeadAttention(model_dim=model_dim, num_heads=num_heads)\n",
    "\n",
    "        self.feed_forward = TransformerFeedForward(model_dim=model_dim, ff_dim=ff_dim)\n",
    "\n",
    "        self.self_attention_layer_norm = nn.LayerNorm(model_dim)\n",
    "        self.cross_attention_layer_norm = nn.LayerNorm(model_dim)\n",
    "        self.ff_layer_norm = nn.LayerNorm(model_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self,\n",
    "                x: torch.Tensor,\n",
    "                encoder_out: torch.Tensor,\n",
    "                encoder_mask: torch.LongTensor = None,\n",
    "                decoder_mask: torch.LongTensor = None):\n",
    "        \"\"\"\n",
    "        Applique les couches d'attention, de normalisation et de r√©seau de neurones\n",
    "        sur l'entr√©e\n",
    "\n",
    "        Args :\n",
    "            x : Entr√©e du d√©codeur correspondant √† la s√©quence de sortie d√©cal√©e vers\n",
    "            la droite\n",
    "            Taille : [batch_size, seq_length, model_dim]\n",
    "\n",
    "            encoder_output : Sortie de l'encodeur utilis√© pour la couche de cross-\n",
    "            attention\n",
    "            Taille : [batch_size, seq_length, model_dim]\n",
    "\n",
    "            encoder_mask : Masque qui cache certains jetons dans la s√©quence d'entr√©e.\n",
    "            Par exemple, les jetons [PAD] seront cach√©s puisqu'ils ne sont utilis√©s que\n",
    "            pour aggrandir les s√©quences jusqu'√† la fen√™tre de contexte du Transformer\n",
    "            Taille : [batch_size, seq_length, seq_length]\n",
    "\n",
    "            decoder_mask : Masque qui cache certains jetons dans la s√©quence de sortie.\n",
    "            Ce masque est notamment utilis√© dans le d√©codeur pour s'assurer que le\n",
    "            transformer n'ait pas acc√®s aux futurs jetons lorsqu'il essaie de pr√©dire\n",
    "            le prochain jeton.\n",
    "            Taille : [batch_size, seq_length, seq_length]\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        pass\n",
    "        # END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnhBKIIU9Fxw"
   },
   "source": [
    "La classe `Decoder` repr√©sente toutes les couches du d√©codeur. La fonction `forward` de cette classe doit appeler chacune des couches (`DecoderLayer`) une √† la suite de l'autre en passant √† la couche courante la sortie de la couche pr√©c√©dente. Les attributs `encoder_out`, `encoder_mask` et `decoder_mask` sont partag√©s entre toutes les couches `DecoderLayer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "id": "MdjZSpzd2tRV"
   },
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, num_layers: int, model_dim: int, ff_dim: int, dropout_rate: int = 0.3, num_heads=8) -> None:\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.layers = nn.ModuleList(\n",
    "            [DecoderLayer(\n",
    "                model_dim=model_dim,\n",
    "                ff_dim=ff_dim,\n",
    "                dropout_rate=dropout_rate,\n",
    "                num_heads=num_heads,\n",
    "            ) for _ in range(num_layers)]\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor, encoder_out: torch.Tensor, encoder_mask: torch.LongTensor = None, decoder_mask: torch.LongTensor = None):\n",
    "        \"\"\"\n",
    "        Applique toutes les couches du d√©codeur cons√©cutivement\n",
    "\n",
    "        Args :\n",
    "            x : Tenseur d'entr√©e du d√©codeur correspondant √† la s√©quence de sortie\n",
    "            Taille : [batch_size, seq_length, model_dim]\n",
    "\n",
    "            encoder_out : Tenseur contenant la s√©quence d'entr√©e encod√©e par l'encodeur\n",
    "\n",
    "            encoder_mask : Tenseur contenant le masque qui sera utilis√© par l'encodeur\n",
    "            pour cacher certains jetons (notamment les jetons [PAD])\n",
    "\n",
    "            decoder_mask : Tenseur contenant le masque qui sera utilis√© par le d√©codeur\n",
    "            pour cacher certains jetons (notamment les jetons [PAD] et les jetons futurs)\n",
    "\n",
    "        Returns :\n",
    "        Tenseur apr√®s avoir appliqu√©s les couches dans l'encodeur de taille\n",
    "        [batch_size, seq_length, model_dim]\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        pass\n",
    "        # END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rqH-GpDP9Fxw"
   },
   "source": [
    "#### 4.5 Transformer (8 points)\n",
    "\n",
    "L'architecture du Transformer est maintenant pr√™te √† √™tre assembl√©e. En utilisant les diff√©rentes couches que vous avez impl√©ment√©es, compl√©tez la fonction `forward` de la classe `Transformer` qui, √† partir de l'entr√©e de l'encodeur et du d√©codeur, g√©n√®re la sortie du d√©codeur. L'architecture, telle que pr√©sent√©e pr√©c√©demment, correspond √† la figure suivante :\n",
    "\n",
    "![Transformer](images/transformer.png)\n",
    "\n",
    "La fonction forward doit passer les entr√©es √† l'encodeur et au d√©codeur pour pouvoir g√©n√©rer une pr√©diction en fonction d'une entr√©e en appliquant les bons masques et encodages de position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {
    "id": "UMWUXGRC2tRV"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class TransformerConfig:\n",
    "\n",
    "    model_dim: int = 512\n",
    "    ff_dim: int = 2048\n",
    "\n",
    "    nb_encoder: int = 6\n",
    "    nb_decoder: int = 6\n",
    "\n",
    "    num_heads: int = 8\n",
    "\n",
    "    max_seq_length: int = MAX_LENGTH\n",
    "    vocab_size: int = VOCAB_SIZE\n",
    "    device: str = 'cpu'\n",
    "    pad_token_id: int = 0\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, config: TransformerConfig) -> None:\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.enc_embeddings = nn.Embedding(\n",
    "            num_embeddings=config.vocab_size,\n",
    "            embedding_dim=config.model_dim\n",
    "        )\n",
    "\n",
    "        self.dec_embeddings = nn.Embedding(\n",
    "            num_embeddings=config.vocab_size,\n",
    "            embedding_dim=config.model_dim\n",
    "        )\n",
    "\n",
    "        self.positional_embeddings = PositionalEmbedding(\n",
    "            max_seq_length=config.max_seq_length,\n",
    "            embedding_dim=config.model_dim\n",
    "        )\n",
    "\n",
    "        self.encoder = Encoder(config.nb_encoder, config.model_dim, config.ff_dim, num_heads=config.num_heads)\n",
    "        self.decoder = Decoder(config.nb_decoder, config.model_dim, config.ff_dim, num_heads=config.num_heads)\n",
    "\n",
    "        self.linear_projection = nn.Linear(config.model_dim, config.vocab_size)\n",
    "        self.device = config.device\n",
    "\n",
    "    def generate_mask(self, src, tgt):\n",
    "        \"\"\"\n",
    "        G√©n√®re les masques d'attentions pour l'encodeur et le d√©codeur du transformer en\n",
    "        se basant sur le jeton de pad donn√© dans la configuration du transformer. Dans le\n",
    "        cas du d√©codeur, un masque causal est √©galement calcul√© pour emp√™cher les jetons\n",
    "        de porter leur attention sur les jetons futurs\n",
    "\n",
    "        Args :\n",
    "            src : S√©quence d'entr√©e\n",
    "\n",
    "        \"\"\"\n",
    "        src_mask = (src != self.config.pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "        tgt_mask = (tgt != self.config.pad_token_id).unsqueeze(1).unsqueeze(3)\n",
    "        seq_length = tgt.size(1)\n",
    "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_length, seq_length), diagonal=1)).bool().to(self.device)\n",
    "        tgt_mask = tgt_mask & nopeak_mask\n",
    "        return src_mask, tgt_mask\n",
    "\n",
    "    def forward(self, encoder_x: torch.Tensor, decoder_x: torch.Tensor):\n",
    "        \"\"\"\n",
    "        G√©n√®re la sortie du d√©codeur √©tant donn√© une entr√©e pour l'encodeur et une entr√©e\n",
    "        pour le d√©codeur\n",
    "\n",
    "        Args :\n",
    "            encoder_x : Tenseur d'entr√©e de l'encodeur\n",
    "            Taille : [batch_size, seq_length]\n",
    "\n",
    "            decoder_x : Tenseur d'entr√©e du d√©codeur\n",
    "            Taille : [batch_size, seq_length]\n",
    "\n",
    "        Returns :\n",
    "        Sortie du d√©codeur correspondant au pr√©dictions du jeton le plus proche. Attention,\n",
    "        n'appliquez pas softmax sur ces pr√©dictions. Ce tenseur devrait avoir une taille de\n",
    "        [batch_size, seq_length, vocab_size]\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO\n",
    "        pass\n",
    "        # END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09w2xPmk9Fxw"
   },
   "source": [
    "### 5. Padding et coupure (5 points)\n",
    "\n",
    "Il est important, lors de l'entra√Ænement que toutes les s√©quences aient la m√™me longueur de texte, car le Transformer prend toujours un nombre fixe de jetons. Cette taille correspond √† sa fen√™tre de contexte. Afin de s'assurer que toutes les s√©quences de texte dans un lot d'entra√Ænement soient de la m√™me longueur, nous allons couper les s√©quences trop longues et ajouter des jetons √† celles qui sont trop courtes. Le jeton qui sera ajout√© sera le jeton de \"padding\" du tokenizer (`[PAD]`).\n",
    "\n",
    "Compl√©tez la fonction `tokenize` de la classe `DataCollator` qui s'occupe d'appeler le tokenizer avec les textes pass√©s en param√®tre en sp√©cifiant les param√®tres suivants :\n",
    "- `padding`: `\"max_length\"` (Attention on veut la cha√Æne de charact√®re `\"max_length\"`, pas la variable)\n",
    "- `truncation`: `True`\n",
    "- `max_length`: La taille maximale pass√©e dans le constructeur\n",
    "- `return_tensors`: `pt`\n",
    "- `return_token_type_ids`: `False`\n",
    "- `add_special_tokens`: `True`\n",
    "\n",
    "Compl√©tez la fonction `__call__` qui s'occupe de prendre un lot de donn√©es (le lot/batch correspondant √† un sous-ensemble de l'ensemble d'entra√Ænement) et retourne les entr√©es de l'encodeur et du d√©codeur ainsi que la sortie du d√©codeur du Transformer. Vous devrez prendre le lot pass√© en param√®tre et transformer les colonnes `sentence` et `concepts` du lot en jetons. Les jetons de la colonne `sentence` seront envoy√©s √† l'encodeur et les jetons de la colonne `concepts` seront envoy√©s au d√©codeur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "id": "FgvbPOmY9Fxw"
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union\n",
    "\n",
    "\n",
    "class DataCollator:\n",
    "    def __init__(self, tokenizer: Tokenizer, max_length: int, device: str = 'cpu') -> None:\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.device = device\n",
    "\n",
    "        self.pad_token_id = tokenizer.pad_token_id\n",
    "        self.bos_token_id = tokenizer.bos_token_id\n",
    "        self.eos_token_id = tokenizer.eos_token_id\n",
    "\n",
    "    def tokenize(self, texts):\n",
    "        \"\"\"\n",
    "        Transforme la s√©quence de textes en s√©quence d'indice de jetons\n",
    "\n",
    "        Args :\n",
    "            texts : Textes √† transformer\n",
    "\n",
    "        Returns :\n",
    "        Indices des jetons des textes\n",
    "        \"\"\"\n",
    "        # TODO\n",
    "        pass\n",
    "        # END TODO\n",
    "\n",
    "    def __call__(self, batch: List[Dict[str, Union[str, int]]]) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Transforme une batch contenant les colonnes target et concepts en les envoyant au tokenizer\n",
    "        et pr√©parant les jetons aux entr√©es et sorties de l'encodeur et du d√©codeur\n",
    "        \"\"\"\n",
    "        # TODO :\n",
    "        encoder = None # ids des jetons des entr√©es \n",
    "        decoder = None # ids des jetons des sorties\n",
    "        # END TODO\n",
    "\n",
    "        \"\"\"\n",
    "        Dans le d√©codeur, la s√©quence attendue est d√©cal√©e vers la droite d'un jeton √† l'entr√©e du d√©codeur.\n",
    "        Par exemple, si on veut faire de la traduction, nous pourrions avoir la s√©quence suivante :\n",
    "\n",
    "        La pomme est verte -> The apple is green\n",
    "\n",
    "        Les valeurs du dictionnaire de retour serait donc :\n",
    "\n",
    "        +-------------+--------+--------+--------+--------+--------+--------+\n",
    "        |   Valeur    | Jeton1 | Jeton2 | Jeton3 | Jeton4 | Jeton5 | Jeton6 |\n",
    "        +-------------+--------+--------+--------+--------+--------+--------+\n",
    "        | encoder_in  | [BOS]  | la     | pomme  | est    | verte  | [EOS]  |\n",
    "        | decoder_in  | [BOS]  | The    | apple  | is     | green  | [EOS]  |\n",
    "        | decoder_out | The    | apple  | is     | green  | [EOS]  | [PAD]  |\n",
    "        +-------------+--------+--------+--------+--------+--------+--------+\n",
    "\n",
    "        Par exemple, pour le jeton 3, le transformer essaiera de pr√©dire le jeton \"is\" avec comme information\n",
    "        tous les jetons de l'encodeur et tous les jetons pr√©c√©dents le \"is\" ([BOS] The apple).\n",
    "\n",
    "        C'est d'ailleurs √† cause de ce d√©calage que nous enlevons le premier jetons dans 'decoder_out'. Aussi,\n",
    "        le dernier jetons dans 'decoder_in' est enlev√©, car le Transformer ne devrait jamais voir le jeton\n",
    "        de fin de phrase, car cela voudrait dire que la s√©quence est termin√©e. Il devrait seulement le pr√©dire.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            'encoder_in': encoder.to(self.device),\n",
    "            'decoder_in': decoder[:, :-1].to(self.device),\n",
    "            'decoder_out': decoder[:, 1:].to(self.device),\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ms_zfOuw9Fxw",
    "outputId": "301a4393-5e76-4f37-a394-0e3b324cf21c"
   },
   "outputs": [],
   "source": [
    "def test_data_collator():\n",
    "    data_collator = DataCollator(tokenizer=tokenizer, max_length=16)\n",
    "    sample_data = test_dataset[:2]\n",
    "    result = data_collator(CustomDataset(data=sample_data))\n",
    "    print('=' * 100)\n",
    "    print()\n",
    "    print('Encoder in detokenized : \\n\\n', '\\n'.join(tokenizer.batch_decode(result['encoder_in'])))\n",
    "    print()\n",
    "    print('Decoder in detokenized : \\n\\n', '\\n'.join(tokenizer.batch_decode(result['decoder_in'])))\n",
    "    print()\n",
    "    print('Decoder out detokenized : \\n\\n', '\\n'.join(tokenizer.batch_decode(result['decoder_out'])))\n",
    "    print()\n",
    "    print('=' * 100)\n",
    "\n",
    "test_data_collator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ft9grnXB9Fxw"
   },
   "source": [
    "Sortie attendue :\n",
    "```\n",
    "====================================================================================================\n",
    "\n",
    "Encoder in detokenized :\n",
    "\n",
    " [BOS]two elephants standing next to each other in their pen [EOS][PAD][PAD][PAD][PAD]\n",
    "[BOS]A big elephant that is standing there quietly in the pen. [EOS]\n",
    "\n",
    "Decoder in detokenized :\n",
    "\n",
    " [BOS]['elephant'' pen'' stand'] [EOS][PAD][PAD]\n",
    "[BOS]['elephant'' pen'' stand'] [EOS][PAD][PAD]\n",
    "\n",
    "Decoder out detokenized :\n",
    "\n",
    " ['elephant'' pen'' stand'] [EOS][PAD][PAD][PAD]\n",
    "['elephant'' pen'' stand'] [EOS][PAD][PAD][PAD]\n",
    "\n",
    "====================================================================================================\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GrA7t_Xy9Fxw"
   },
   "source": [
    "### 6. Cr√©ation de lots (Batching)\n",
    "\n",
    "Nous allons utiliser la classe `DataLoader` de PyTorch pour charger les donn√©es en \"batchs\". La classe `DataCollator` sera pass√©e en param√®tre lors du chargement des donn√©es pour automatiquement transformer le texte en jetons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {
    "id": "lptJkGPm9Fxw"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 256\n",
    "TEST_BATCH_SIZE = 128\n",
    "\n",
    "collator = DataCollator(tokenizer=tokenizer, max_length=MAX_LENGTH, device=DEVICE)\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collator)\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collator)\n",
    "test_loader = DataLoader(test_dataset, batch_size=TEST_BATCH_SIZE, shuffle=False, collate_fn=collator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7R_9-0va9Fxw"
   },
   "source": [
    "### 7. Entra√Ænement (26 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9BCgDQaf9Fxw"
   },
   "source": [
    "#### 7.1 Boucle d'entra√Ænement (20 points)\n",
    "\n",
    "Pour pouvoir g√©n√©rer des pr√©dictions qui ont du sens, il faut entra√Æner le mod√®le √† effectuer ces pr√©dictions √† partir des donn√©es d'entra√Ænement. Pour cela, la classe `Trainer` sera utilis√©e. Elle permettra au mod√®le, √† partir de donn√©es d'entra√Ænement, d'apprendre la bonne combinaison de param√®tres qui effectue les meilleures pr√©dictions. Nous validerons ensuite les pr√©dictions avec l'ensemble de test. L'ensemble de validation sera utilis√© durant l'entra√Ænement pour s'assurer que le mod√®le apprend bien.\n",
    "\n",
    "Compl√©ter les fonctions `train_epoch` et `validation_epoch` de la classe `Trainer` pour permettre au transformer pass√© en param√®tre du constructeur d'√™tre entra√Æn√© avec les donn√©es d'entra√Ænement.\n",
    "\n",
    "La fonction `train_epoch` doit :\n",
    "- Parcourir toutes les lots (batchs) d'entra√Ænement et pour chaque lot :\n",
    "  - Entra√Æner le mod√®le pour ce lot en √©valuant la fonction de perte et mettant √† jour les param√®tres en fonction des gradients\n",
    "- Calculer la perte d'entra√Ænement moyenne\n",
    "- Mettre la perte d'entra√Ænement dans un objet de la classe `History`\n",
    "\n",
    "La fonction `validation_epoch` doit :\n",
    "- Parcourir tous les lots de validation et pour chaque lot :\n",
    "  - √âvaluer le mod√®le sur ce lot en √©valuant la fonction de perte\n",
    "- Calculer la perte de validation moyenne\n",
    "- Mettre la perte de validation dans un objet de la classe `History`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {
    "id": "HhqDUPeB2tRV"
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "class History:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.t_loss = []\n",
    "        self.v_loss = []\n",
    "        self.time_to_train = -1\n",
    "\n",
    "class Trainer:\n",
    "\n",
    "    def __init__(self,\n",
    "                 transformer: Transformer,\n",
    "                 train_loader: DataLoader,\n",
    "                 val_loader: DataLoader,\n",
    "                 num_epochs: int,\n",
    "                 loss_function = None,\n",
    "                 device='cpu',\n",
    "                 saving_path='model') -> None:\n",
    "        \"\"\"\n",
    "        Args :\n",
    "            transformer: Mod√®le qui sera entra√Æn√©\n",
    "\n",
    "            train_loader: Objet contenant les donn√©es d'entra√Ænement en batch\n",
    "\n",
    "            val_loader: Objet contenant les donn√©es de validation en batch\n",
    "\n",
    "            num_epochs: Nombre d'√©tape d'entra√Ænement (une √©tape √©quivaut √†\n",
    "            parcourir toutes les donn√©es une fois)\n",
    "\n",
    "            loss_function: Fonction de perte utilis√©e lors de l'entra√Ænement. Si\n",
    "            le param√®tre est laiss√© √† `None`, la fonction d'entropie crois√©e sera\n",
    "            utilis√©e en ignorant les jetons de pad (retrouv√©s avec la config du mod√®le)\n",
    "\n",
    "            device: Machine sur laquelle le mod√®le sera entra√Æn√©\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = transformer\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.num_epochs = num_epochs\n",
    "        self.device = device\n",
    "        self.saving_path = saving_path\n",
    "\n",
    "        self.optimizer = O.Adam(self.model.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "        if loss_function is None:\n",
    "            self.loss_function = nn.CrossEntropyLoss(ignore_index=transformer.config.pad_token_id).to(self.device)\n",
    "        else:\n",
    "            self.loss_function = loss_function\n",
    "\n",
    "    def compute_loss(self, logits: torch.Tensor, labels: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Trouve la valeur de la fonction de perte (self.loss_function)\n",
    "        √©tant donn√© les probabilit√©s (logits) pr√©dits et les vraies\n",
    "        valeurs (labels)\n",
    "\n",
    "        Args :\n",
    "            logits:     Probabilit√©s pr√©dites par le mod√®le sur le prochain\n",
    "                        jeton pour chacun des jetons de la s√©quence\n",
    "                        Tenseur de taille : [batch_size, seq_length, vocab_size]\n",
    "\n",
    "            labels:     Jetons qui devraient √™tre pr√©dis comme les prochains\n",
    "                        jetons pour chaque jeton de la s√©quence\n",
    "                        Tenseur de taille : [batch_size, seq_length]\n",
    "        \"\"\"\n",
    "\n",
    "        _, _, vocab_size = logits.shape\n",
    "        return self.loss_function(logits.contiguous().view(-1, vocab_size), labels.contiguous().view(-1))\n",
    "\n",
    "    def train_epoch(self, history):\n",
    "        \"\"\"\n",
    "        Entra√Æne le mod√®le sur tous les lots du `self.train_loader` et calcule\n",
    "        la perte d'entra√Ænement moyen en l'ajoutant √† l'objet history pass√© en\n",
    "        param√®tre\n",
    "\n",
    "        Args :\n",
    "            history :   Objet contenant les statistiques d'entra√Ænement d'un mod√®le\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO\n",
    "        pass\n",
    "        # END TODO\n",
    "\n",
    "    def validation_epoch(self, history):\n",
    "        \"\"\"\n",
    "        √âvalue le mod√®le sur tous les lots du `self.val_loader` et calcule\n",
    "        la perte de validation moyen en l'ajoutant √† l'objet history pass√© en\n",
    "        param√®tre\n",
    "\n",
    "        Args :\n",
    "            history :   Objet contenant les statistiques d'entra√Ænement d'un mod√®le\n",
    "        \"\"\"\n",
    "\n",
    "        # TODO\n",
    "        pass\n",
    "        # END TODO\n",
    "\n",
    "\n",
    "    def train(self) -> History:\n",
    "        \"\"\"\n",
    "        Entra√Æne `self.model` en utilisant les donn√©es de `self.train_loader`\n",
    "\n",
    "        Returns :\n",
    "        Historique contenant les perte d'entra√Ænement et de validation moyennes\n",
    "        pour chaque √©tape (epoch) d'entra√Ænement\n",
    "        \"\"\"\n",
    "        history = History()\n",
    "        start = time.time()\n",
    "        self.model.to(self.device)\n",
    "\n",
    "\n",
    "        for epoch in range(self.num_epochs):\n",
    "\n",
    "            self.train_epoch(history)\n",
    "            self.validation_epoch(history)\n",
    "\n",
    "            print(f'Epoch {epoch} / {self.num_epochs} : train_loss = {history.t_loss[-1]}, val_loss = {history.v_loss[-1]}')\n",
    "\n",
    "            if epoch > 0 and epoch % 10 == 0:\n",
    "                self.save(f'{self.saving_path}_{epoch}.pt')\n",
    "\n",
    "        end = time.time()\n",
    "        history.time_to_train = end - start\n",
    "        self.save(f'{self.saving_path}_{self.num_epochs}.pt')\n",
    "        return history\n",
    "\n",
    "    def save(self, path: str):\n",
    "        \"\"\"\n",
    "        Saves the model in the specified path\n",
    "        \"\"\"\n",
    "        torch.save(self.model.state_dict(), path)\n",
    "\n",
    "    @staticmethod\n",
    "    def load(config: TransformerConfig, path: str):\n",
    "        \"\"\"\n",
    "        Loads the model from the specified path\n",
    "        \"\"\"\n",
    "        model = Transformer(config)\n",
    "        model.load_state_dict(torch.load(path))\n",
    "        return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rh33P37P9Fxx"
   },
   "source": [
    "#### 7.2 Nombre de param√®tres (2 points)\n",
    "Compl√©tez maintenant la fonction `count_parameters` permettant de calculer le nombre de param√®tres du mod√®le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mUnb2fRp2tRV",
    "outputId": "f435149f-f5fd-4a75-ebc0-840c11723fc5"
   },
   "outputs": [],
   "source": [
    "def count_parameters(model):\n",
    "    \"\"\"\n",
    "    Calcule le nombre de param√®tres que l'on peut entra√Æner dans un mod√®le\n",
    "\n",
    "    Args :\n",
    "        model : Mod√®le dont on veut savoir le nombre de param√®tres\n",
    "\n",
    "    Returns :\n",
    "    Nombre de param√®tres\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    pass\n",
    "    # END TODO\n",
    "\n",
    "config = TransformerConfig()\n",
    "config.device = DEVICE\n",
    "model = Transformer(config)\n",
    "\n",
    "print('Nombre de param√®tres : ', count_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VyvX8Loe2tRV"
   },
   "source": [
    "#### 7.3 Entra√Ænement (4 points)\n",
    "Entra√Ænez maintenant le mod√®le pour 30 √©poques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cf6Q-9v12tRc",
    "outputId": "eb823537-5c8f-4eda-c058-0a7f475777ed"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "history = None # Mettez le r√©sultat de l'entra√Ænement dans cette variable\n",
    "# END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Historique d'entra√Ænement (5 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paXl6ZeL9Fxx"
   },
   "source": [
    "#### 8.1 Graphique (2 points)\n",
    "Compl√©tez la fonction `show_history` qui affiche l'historique d'entra√Ænement (perte d'entra√Ænement et perte de validation par √©poque, utilisez des pas de 5 √©poques) du mod√®le dans un graphe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "w9oPTKOB9Fxx",
    "outputId": "59fd18df-2fb1-4e0d-cccc-9d18d40911cd"
   },
   "outputs": [],
   "source": [
    "def show_history(history: History):\n",
    "    \"\"\"\n",
    "    Affiche l'historique d'entra√Ænement du mod√®le dans un graphique\n",
    "\n",
    "    Args :\n",
    "        history : Objet contenant les pertes d'entra√Ænement et de\n",
    "        validation de chaque √©tape d'entra√Ænement\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    pass\n",
    "    # END TODO\n",
    "\n",
    "show_history(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9LwB7-VX9Fxx"
   },
   "source": [
    "#### 8.2 Est-ce que le mod√®le semble √™tre en sur-apprentissage ? Pourquoi ? Que feriez-vous pour r√©soudre ce probl√®me ? (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IndyxKRWBFiQ"
   },
   "source": [
    "#### 8.3 Lors de l'entra√Ænement, un mod√®le a √©t√© sauvegard√© √† chaque 10 √©poques. En vous basant sur le graphique de la fonction de perte durant l'entra√Ænement, choisissez la sauvegarde du mod√®le qui n'est pas en sur-apprentissage et chargez le pour l'√©valuation √† l'aide de la fonction `load` de la classe `Trainer`. Chargez √©galement le dernier mod√®le sauvegard√© pour comparer les r√©sultats. (1 point)\n",
    "\n",
    "Nous d√©noterons le mod√®le arr√™t√© avant le sur-apprentissage comme √©tant : `stopped_model`\n",
    "\n",
    "Nous d√©noterons le dernier mod√®le sauvegard√© comme √©tant : `last_model`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "id": "O96q-EK2BZr9"
   },
   "outputs": [],
   "source": [
    "# TODO\n",
    "\n",
    "# Mettez vos deux mod√®les entra√Æn√©s dans ces variables pour la suite\n",
    "stopped_model = None\n",
    "last_model = None\n",
    "\n",
    "# END TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W95hjKY19Fxx"
   },
   "source": [
    "### 9. √âvaluation (20 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oM_j2M0A9Fxx",
    "tags": []
   },
   "source": [
    "#### 9.1 G√©n√©ration (10 points)\n",
    "Maintenant que le mod√®le est entra√Æn√©, nous pouvons tester ses g√©n√©rations. Compl√©tez la fonction `generate` qui g√©n√®re, pour un lot de donn√©es, les pr√©dictions d'un mod√®le sur les concepts cl√©s de la phrase donn√©e en param√®tre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "id": "UmoO5WYT9Fxx"
   },
   "outputs": [],
   "source": [
    "def generate(model: Transformer, encoder_in: torch.tensor, bos_token_id: int, eos_token_id: int, max_length=MAX_LENGTH):\n",
    "    \"\"\"\n",
    "    G√©n√®re les pr√©dictions d'un mod√®le pour des phrases donn√©es en param√®tre. Pour cela, vous devez\n",
    "    initialiser un tenseur X contenant les jetons de d√©but de phrase (bos_token_id). Ensuite, ce\n",
    "    tenseur sera pass√© comme entr√©e au d√©codeur avec `encoder_in` comme entr√©e √† l'encodeur. Le mod√®le\n",
    "    g√©n√®rera un jeton en sortie qui sera le jeton le plus probable √©tant donn√© le jeton de d√©but de\n",
    "    phrase et la s√©quence pass√©e dans l'encodeur. Ce jeton devra √™tre concat√©n√© au tenseur initial X\n",
    "    pour former une s√©quence de deux jetons. Cette nouvelle s√©quence est ensuite r√©envoy√©e au d√©codeur.\n",
    "    Un jeton en sortie sera g√©n√©r√© correspondant au jeton le plus probable √©tant donn√© la s√©quence de\n",
    "    deux jetons et l'entr√©e de l'encodeur. Le nouveau jeton est concat√©n√© au tenseur X et l'op√©ration\n",
    "    est r√©p√©t√©e jusqu'√† ce que le d√©codeur g√©n√®re le jeton de fin de phrase.\n",
    "\n",
    "    Args :\n",
    "        model : Mod√®le effectuant les pr√©dictions\n",
    "\n",
    "        inputs : Tenseur contenant les phrases d'entr√©es sous forme d'indices de jetons\n",
    "\n",
    "        bos_token_id : Jeton d'entr√©e du tokenizer utilis√© pour initialiser le tenseur de g√©n√©ration\n",
    "\n",
    "        eos_token_id : Jeton de fin du tokenizer utilis√© pour d√©tecter la fin d'une s√©quence\n",
    "\n",
    "        max_length : Nombre maximal de jetons qui doivent √™tre g√©n√©r√©s par le mod√®le\n",
    "\n",
    "    Returns :\n",
    "    G√©n√©ration du mod√®le de chacune des phrases en entr√©e\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    pass\n",
    "    # END TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "id": "5t_4T4Eg9Fxx"
   },
   "outputs": [],
   "source": [
    "def get_predictions(model, tokenizer):\n",
    "\n",
    "    bos_token_id = tokenizer(\"\")['input_ids'][0]\n",
    "    eos_token_id = tokenizer(\"\")['input_ids'][1]\n",
    "\n",
    "    sentences = []\n",
    "    predictions = []\n",
    "    ground_truth = []\n",
    "    for test_data in test_loader:\n",
    "\n",
    "        inputs = test_data['encoder_in']\n",
    "        result = generate(model, inputs, bos_token_id, eos_token_id)\n",
    "\n",
    "        sentences.extend(tokenizer.batch_decode(test_data['encoder_in'], skip_special_tokens=True))\n",
    "        ground_truth.extend(tokenizer.batch_decode(test_data['decoder_out'], skip_special_tokens=True))\n",
    "        predictions.extend(tokenizer.batch_decode(result, skip_special_tokens=True))\n",
    "\n",
    "    return sentences, ground_truth, predictions\n",
    "\n",
    "sentences_stopped, ground_truth_stopped, predictions_stopped = get_predictions(stopped_model, tokenizer)\n",
    "sentences_last, ground_truth_last, predictions_last = get_predictions(last_model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "id": "2e8pOemw9Fxx"
   },
   "outputs": [],
   "source": [
    "predictions_stopped = pd.DataFrame({'sentences': sentences_stopped, 'ground_truth': ground_truth_stopped, 'predictions': predictions_stopped})\n",
    "predictions_last = pd.DataFrame({'sentences': sentences_last, 'ground_truth': ground_truth_last, 'predictions': predictions_last})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vGOifYaW9Fxx"
   },
   "source": [
    "#### 9.2 √âvaluation naive de la g√©n√©ration (1 point)\n",
    "Nous allons d'abord mesurer l'efficacit√© de notre mod√®le pour extraire les concepts √† l'aide d'une m√©trique de correspondance exacte (exact match). Pour cela, nous allons mesurer le nombre de g√©n√©rations (`predictions`) qui sont identiques √† celles qui sont attendues dans l'ensemble de r√©f√©rence (`ground truth`) et diviser le tout par le nombre totaux d'√©l√©ments dans l'ensemble de r√©f√©rence. Ainsi, la m√©trique EM (exact match) √©quivaut √† :\n",
    "\n",
    "$$\\text{EM} = \\frac{\\text{Nombre d'√©l√©ments identiques entre l'ensemble g√©n√©r√© et l'ensemble de r√©f√©rence}}{\\text{Nombre d'√©l√©ments dans l'ensemble de r√©f√©rence}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3owRHw1D9Fxx",
    "outputId": "2ebbe3d6-4f61-4edc-f148-2ebc1de48443"
   },
   "outputs": [],
   "source": [
    "def compute_em_score(data):\n",
    "    \"\"\"\n",
    "    √âvalue la m√©trique EM du mod√®le en utilisant la m√©trique BLEU\n",
    "    Args :\n",
    "        - data : DataFrame contenant les colonnes predictions et ground_truth\n",
    "\n",
    "    Returns :\n",
    "    La m√©trique EM du mod√®le en pourcentage\n",
    "    \"\"\"\n",
    "    # TODO\n",
    "    pass\n",
    "    # END TODO\n",
    "\n",
    "print(f\"La m√©trique EM du mod√®le arr√™t√© selon la fonction de perte est {compute_em_score(predictions_stopped):.2f} %\")\n",
    "print(f\"La m√©trique EM du dernier mod√®le est {compute_em_score(predictions_last):.2f} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8TlJ99y99Fxy"
   },
   "source": [
    "#### 9.3 Quels probl√®mes voyez-vous avec cette mani√®re de calculer la performance du mod√®le ? (1 point)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v5TGstQP9Fxy"
   },
   "source": [
    "#### 9.4 M√©trique BLEU (4 points)\n",
    "Pour pallier √† ce probl√®me, la m√©trique BLEU sera utilis√©e puisqu'elle est bas√©e sur le nombre de n-gramme qui sont pr√©sents dans les pr√©dictions et les s√©quences voulues. La formule est donn√©e ci-dessous :\n",
    "$$BLEU = BP * exp \\Big( \\sum_{n=1}^{N} w_n \\log p_n \\Big)$$\n",
    "\n",
    "Consid√©rant que $r$ est la phrase de r√©f√©rence (voulue) et $c$ la phrase g√©n√©r√©e (candidate), $p_n$ est la pr√©cision modifi√©e pour le n-gramme (correspondant au ratio de la fr√©quence maximum du n-gramme dans la phrase de r√©f√©rence par la fr√©quence du n-gramme): \n",
    "$$p_n = \\frac{\\sum_{\\text{n-gramme} \\in c} \\min\\Big( \\max_{r} \\text{Count$_r$(n-gramme)}, \\text{Count$_c$(n-gramme)} \\Big)}{\\sum_{\\text{n-gramme} \\in c} \\text{Count$_c$(n-gramme)}}$$\n",
    "\n",
    "Notez que le $\\max_{r}$ est pr√©sent ici car BLEU accepte plusieurs phrases de r√©f√©rence pour une m√™me phrase g√©n√©r√©e. Cependant, dans notre cas, il y a seulement une seule phrase de r√©f√©rence.\n",
    "\n",
    "Posons ensuite $|r|$ comme le nombre de mots dans la phrase cible et $|c|$ comme le nombre de mots dans la phrase pr√©dite. Si $c>r$, alors BP vaut 1. Sinon $$BP = exp(1 - \\frac{|r|}{|c|})$$\n",
    "\n",
    "Les valeurs des poids $w_n$ est ce qui donne les diff√©rentes variations de la m√©trique BLEU. Dans notre cas, la m√©trique BLEU-1 sera utilis√©e. La valeur maximale du score BLEU est 1 et la valeur minimale est 0.\n",
    "\n",
    "Vous pouvez utiliser la fonction `sentence_bleu` de `nltk` pour calculer votre score BLEU. N'oubliez pas d'enlever les apostrophes et les crochets des g√©n√©rations avant votre calcul."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dChq1yTK9Fxy",
    "outputId": "570a935f-f889-42a4-8dc0-1018062be2d3"
   },
   "outputs": [],
   "source": [
    "def compute_bleu(predictions):\n",
    "    \"\"\"\n",
    "    √âvalue la pr√©cision du mod√®le en utilisant la m√©trique BLEU\n",
    "    Args :\n",
    "        - data : DataFrame contenant les colonnes predictions et ground_truth\n",
    "\n",
    "    Returns :\n",
    "        La moyenne du score BLEU\n",
    "    \"\"\"\n",
    "    weights = (1, 0, 0, 0) # Use Bleu-1\n",
    "    # TODO\n",
    "    pass\n",
    "    # END TODO\n",
    "\n",
    "print(f\"Le score BLEU du mod√®le arr√™t√© selon la fonction de perte est {compute_bleu(predictions_stopped):.2f}\")\n",
    "print(f\"Le score BLEU du dernier mod√®le est {compute_bleu(predictions_last):.2f}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rZ2DjH3ZeRVO"
   },
   "source": [
    "#### 9.5 Quel est l'avantage d'utiliser la m√©trique BLEU par rapport √† la m√©trique EM bas√©e sur la comparaison de cha√Æne de caract√®res ? (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f06F1OICfO7_"
   },
   "source": [
    "#### 9.6 Que remarquez-vous par rapport aux performances du mod√®le arr√™t√© avant le sur-apprentissage selon la fonction de perte versus celles du dernier mod√®le ? Quelles sont les raisons qui peuvent expliquer cela ? (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C2cXyEuA9Fxy"
   },
   "source": [
    "### 10. Exploration (4 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U99e4YjL9Fxy"
   },
   "source": [
    "#### 10.1 Explorez les g√©n√©rations actuelles de votre mod√®le et ressortez 2 probl√®mes que le mod√®le a lors de la g√©n√©ration qui font diminuer son score BLEU. (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JfdKeifU9Fxy"
   },
   "source": [
    "#### 10.2 Am√©lioration des m√©triques (2 points)\n",
    "L'exemple ci-dessous montre 2 g√©n√©rations de 2 mod√®les pour la phrase \"A dog is eating a flower\". Bien que la g√©n√©ration du mod√®le 2 soit beaucoup plus proche s√©mantiquement de la r√©f√©rence que celle du mod√®le 1, la m√©trique BLEU retourne 0.0 dans les deux cas. En effet, puisque la m√©trique ne compare que les n-grammes, la pr√©diction du mod√®le 2 est quand m√™me de 0, car aucun des mots dans la pr√©diction ne sont pr√©sents dans les concepts voulus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gd8Y64Pv9Fxy",
    "outputId": "475dccf0-52c5-4025-a083-10463fab3cf9"
   },
   "outputs": [],
   "source": [
    "ground_truth = \"['dog' 'eat' 'flower']\"\n",
    "\n",
    "prediction_1 = \"['car' 'hit' 'person']\"\n",
    "preds = pd.DataFrame({'predictions': [prediction_1], 'ground_truth': [ground_truth]})\n",
    "\n",
    "print(f\"Concepts voulus {ground_truth}\")\n",
    "print(f\"Pr√©diction du mod√®le 1 : {prediction_1}\")\n",
    "print(f\"BLEU : {compute_bleu(preds):.2f}\")\n",
    "\n",
    "prediction_2 = \"['animal' 'eating' 'plant']\"\n",
    "preds = pd.DataFrame({'predictions': [prediction_2], 'ground_truth': [ground_truth]})\n",
    "\n",
    "print(f\"Pr√©diction du mod√®le 2 : {prediction_2}\")\n",
    "print(f\"BLEU : {compute_bleu(preds):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OLVBUJ2e9Fxy"
   },
   "source": [
    "Proposez (sans l'impl√©menter) une meilleure m√©trique pour √©valuer les mod√®les face √† cette t√¢che d'extraction qui donnerait un score plus √©lev√© √† la pr√©diction du mod√®le 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Livrables\n",
    "Vous devez remettre votre notebook sur Moodle et Gradescope en ipynb et pdf. Pour Gradescope vous devez associer les num√©ros de questions avec vos r√©ponses dans le pdf gr√¢ce √† l'outil que fournit Gradescope.\n",
    "\n",
    "\n",
    "## √âvaluation \n",
    "Votre TP sera √©valu√© selon les crit√®res suivants :\n",
    "1. Ex√©cution correcte du code et obtention des sorties attendues\n",
    "2. R√©ponses correctes aux questions d'analyse\n",
    "3. Qualit√© du code (noms significatifs, structure, performance, gestion d‚Äôexception, etc.)\n",
    "4. Commentaires clairs et informatifs"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
